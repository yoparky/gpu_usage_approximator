from fastapi import FastAPI, HTTPException
from typing import Dict
from fastapi.middleware.cors import CORSMiddleware

# ìš”ì²­, ë‹µë³€, ë‹µë³€ìš© ì•„í‚¤í…ì²˜ í´ë˜ìŠ¤ ì„í¬íŠ¸
from classes.MemoryEstimation import MemoryEstimation
from classes.ModelArchitecture import ModelArchitecture
from classes.ModelRequest import ModelRequest

# ìœ í‹¸ ì„í¬íŠ¸: í—ˆê¹…í˜ì´ìŠ¤ ì»¨í”¼ê·¸ íŒŒì¼ ë‹¤ìš´, ì¶”ë¡  ë©”ëª¨ë¦¬ 4ê°€ì§€ í•­ëª©
from utils.GetFromHuggingFace import (
    fetch_model_config_from_hf, 
    get_params_from_hf_config, 
    get_model_architecture,
    get_context_length_from_config
)
from utils.CalculateActivation import calculate_activation_memory_inference
from utils.CalculateKVCache import calculate_kv_cache_memory
from utils.CalculateModelParameters import calculate_model_memory
from utils.GeneralUtil import load_model_config, estimate_params_from_name

app = FastAPI(title="vLLM GPU ë©”ëª¨ë¦¬ ê³„ì‚°ê¸° API")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=False,
    allow_methods=["*"],
    allow_headers=["*"],
)
# API ëª¨ë¸ ê´€ë ¨ ì„¤ì •ê°’
PATH_TO_API_DEFINES = "./data/defines.yaml"

# ê¸€ë¡œë²Œ ë””íŒŒì¸
DTYPE_SIZES, QUANTIZATION_FACTORS, PARAM_MAPPINGS = load_model_config()
GPU_CSV_FILE_PATH = "./data/raw_data/gpu_specs_v6.csv"
GPU_JSON_FILE_PATH = "./data/processed_data/test.json"

def convert_max_num_seqs_to_base(max_num_seqs: int):
    # vLLMì˜ Continuous Batching ê¸°ëŠ¥ì€ ê¸°ì¡´ í”„ë ˆì„ì›Œí¬ë“¤ì˜ ë°°ì¹˜ ê¸°ëŠ¥ê³¼ ë‹¤ë¥´ê²Œ ë™ì‘í•©ë‹ˆë‹¤.
    # ì´ë•Œë¬¸ì— ë°°ì¹˜ í•˜ë‚˜ ë‚´ì— ìˆëŠ” ì‹œí€¸ìŠ¤ ìµœëŒ€ ìˆ˜ë¥¼ ì¡°ì ˆ í•˜ëŠ”ë° ì´ ë•Œ í™œì„±í™” ë©”ëª¨ë¦¬ì˜ ì´ëŸ‰ì´
    # ì‹œí€¸ìŠ¤ ìˆ˜ì— ë”°ë¼ ì¦ê°í•©ë‹ˆë‹¤. 0.7.0 ë²„ì „ì˜ ë””í´íŠ¸ max_num_seqs 256ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ì—¬
    # í™œì„±í™” ë©”ëª¨ë¦¬ì˜ ê°’ì˜ ê·¼ì‚¬ê°’ì´ ì •í•´ì ¸ ì´ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœëŒ€ ì‹œí€¸ìŠ¤ ìˆ˜ë¥¼ ê³„ì‚°ì— í™œìš©ì„ í•  ìˆ˜
    # ìˆëŠ” ì‰¬ìš´ ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    # ì´ëŠ” ì‹¤ì¸¡ì„ í†µí•´ ì–»ì€ ë©”ëª¨ë¦¬ ì†Œëª¨ëŸ‰ì„ ê¸°ì¤€ìœ¼ë¡œ í–ˆìŠµë‹ˆë‹¤.

    conversion_ratio = max_num_seqs
    # 128 ì´í•˜ì˜ ê°’ì€ 128ê³¼ ê±°ì˜ ê°™ì€ ì–‘ì˜ ë©”ëª¨ë¦¬ë¥¼ ì†Œëª¨í•˜ëŠ” ê²ƒìœ¼ë¡œ ê´€ì¸¡ë¬ìŠµë‹ˆë‹¤
    if conversion_ratio <= 128:
        conversion_ratio = 0.5
    else: # ì´ì™¸ì˜ ê°’ë“¤ì€ linear ê´€ê³„ë¡œ ì¦ê°í•©ë‹ˆë‹¤.
        conversion_ratio = conversion_ratio / 256

    return conversion_ratio

def calculate_per_gpu_memory(
    tensor_parallel_size: int,
    model_params_memory_gb: float,
    activation_memory_gb: float,
    min_kv_cache_memory_gb: float,
    max_kv_cache_memory_gb: float,
    cuda_overhead_gb: float,
) -> Dict[str, float]:
    """
    í…ì„œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì‚¬ìš©í•  ë•Œ GPUë‹¹ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    ì‹¤ì œ vLLM ë¡œê·¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë¶„í¬ë¥¼ ë” ì˜ ë°˜ì˜í•˜ë„ë¡ ê°œì„ :
    - ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ê°€ ì™„ë²½í•˜ê²Œ ë¶„ì‚°ë˜ì§€ ì•ŠìŒ(ì˜¤ë²„í—¤ë“œ ì¶”ê°€)
    - í™œì„±í™” ë©”ëª¨ë¦¬ëŠ” ê¸°ë³¸ ë° ì‘ì—…ì GPU ê°„ì— ë‹¤ë¦„
    - KV ìºì‹œëŠ” ì‹¤ì œ ëª¨ë¸ ìš”êµ¬ ì‚¬í•­ì„ ê¸°ë°˜ìœ¼ë¡œ í•¨
    - CUDA ì˜¤ë²„í—¤ë“œëŠ” ëª¨ë¸ í¬ê¸°ì— ë”°ë¼ ë‹¤ë¦„
    - ê·¸ë˜í•‘ ì‘ì—…ìœ¼ë¡œ ëª¨ë¸ì„ ì²˜ìŒ ì‹œì‘í•  ë•Œ ë¡œê·¸ìƒ ì¶”ê°€ ì˜¤ë²„í—¤ë“œ ë°œìƒ, ì˜¤ì°¨ìœ¨ì„ ì¶”ê°€í•˜ì—¬ ê·¹ë³µ
    
    Args:
        tensor_parallel_size: í…ì„œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ GPU ìˆ˜
        model_params_memory_gb: GB ë‹¨ìœ„ì˜ ì´ ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ë©”ëª¨ë¦¬
        activation_memory_gb: GB ë‹¨ìœ„ì˜ ì´ í™œì„±í™” ë©”ëª¨ë¦¬
        min_kv_cache_memory_gb: GB ë‹¨ìœ„ì˜ ìµœì†Œ KV ìºì‹œ ë©”ëª¨ë¦¬
        max_kv_cache_memory_gb: GB ë‹¨ìœ„ì˜ ìµœëŒ€ KV ìºì‹œ ë©”ëª¨ë¦¬
        cuda_overhead_gb: GB ë‹¨ìœ„ì˜ CUDA ì˜¤ë²„í—¤ë“œ
        params_billions: ìˆ˜ì‹­ì–µ ë‹¨ìœ„ì˜ ëª¨ë¸ í¬ê¸°
        
    Returns:
        GPUë‹¹ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    if tensor_parallel_size <= 1:        
        return {
            "per_gpu_model_params": model_params_memory_gb,
            "per_gpu_activation": activation_memory_gb,
            "per_gpu_min_kv_cache": min_kv_cache_memory_gb,
            "per_gpu_max_kv_cache": max_kv_cache_memory_gb,
            "per_gpu_cuda_overhead": cuda_overhead_gb,
            "per_gpu_primary_activation": activation_memory_gb,
        }
    
    per_gpu_model_params = (model_params_memory_gb / tensor_parallel_size)
    
    communication_overhead = 0.25 # ì‹¤ì¸¡ ì–´ë¦¼ ê°’, ë¡œê·¸ ìƒ non-torch memory GiB
    total_additional_overhead = communication_overhead # ì°¨ í›„ ì¶”ê°€

    primary_gpu_activation = activation_memory_gb
    # ì°¨í›„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì½”ë“œ
    worker_gpu_activation = activation_memory_gb / (5 + tensor_parallel_size - 1) * (1 + total_additional_overhead) * 1.1 # 1.1ì€ ì˜¤ì°¨ë¥¼ ìœ„í•œ ê³„ìˆ˜ì…ë‹ˆë‹¤ TODO: max_num_seqs_ratio ê³ ë ¤í•´ì„œ ì¶”ì • í•„ìš”
   
    per_gpu_activation = primary_gpu_activation # ê°€ì¥ ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ìš”êµ¬í•˜ëŠ” ì£¼ GPUë¡œ ê°’ ê³„ì‚°
    
    per_gpu_min_kv_cache = min_kv_cache_memory_gb / tensor_parallel_size
    per_gpu_max_kv_cache = max_kv_cache_memory_gb / tensor_parallel_size
    
    parent_gpu_cuda_overhead = cuda_overhead_gb / tensor_parallel_size + communication_overhead # ë³‘ë ¬í™” ì‹œ ì£¼ gpu ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì˜¤ë²„í—¤ë“œ ì¶”ê°€
    # ì°¨í›„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì½”ë“œ
    worker_gpu_cuda_overhead = (parent_gpu_cuda_overhead - communication_overhead) / (5 + tensor_parallel_size - 1) * (1 + total_additional_overhead)
    return {
        "per_gpu_model_params": per_gpu_model_params,
        "per_gpu_activation": per_gpu_activation,
        "per_gpu_min_kv_cache": per_gpu_min_kv_cache,
        "per_gpu_max_kv_cache": per_gpu_max_kv_cache,
        "per_gpu_cuda_overhead": parent_gpu_cuda_overhead,
    }


@app.post("/estimate-gpu-memory", response_model=MemoryEstimation)
def estimate_gpu_memory(request: ModelRequest) -> MemoryEstimation:
    """
    vLLMìœ¼ë¡œ íŠ¹ì • ëª¨ë¸ ì„œë¹™ì„ ìœ„í•œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¶”ì •í•©ë‹ˆë‹¤.
    ê²Œì´í‹°ë“œ ëª¨ë¸(ì˜ˆ: Llama)ì˜ ê²½ìš° ìœ íš¨í•œ Hugging Face í† í°ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    ë‹¤ìŒ ì‚¬í•­ì„ ê³ ë ¤í•©ë‹ˆë‹¤:
    1. ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ ë©”ëª¨ë¦¬
    2. ì¶”ë¡ ì„ ìœ„í•œ í™œì„±í™” ë©”ëª¨ë¦¬
    3. KV ìºì‹œ ë©”ëª¨ë¦¬(PagedAttention ì‚¬ìš©)
    4. CUDA ì»¨í…ìŠ¤íŠ¸ ë° ê¸°íƒ€ ì˜¤ë²„í—¤ë“œ
    """
    estimated_values = []
    missing_values = []

    current_gpu_memory_use_gb = request.curr_gpu_use if request.curr_gpu_use else 0
    current_gpu_memory_size_gb = request.current_gpu_memory_size_gb if request.current_gpu_memory_size_gb else 0

    if request.tensor_parallel_size <= 0:
        raise HTTPException(status_code=400, detail="tensor_parallel_sizeëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    # Hugging Face í† í° ì‚¬ìš© ì—¬ë¶€ í™•ì¸
    token = request.token
    if token:
        print(f"ğŸ” {request.model_name}ì— ì ‘ê·¼í•˜ê¸° ìœ„í•œ ì¸ì¦ í† í°ì´ ì œê³µë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"â„¹ï¸ {request.model_name}ì— ì ‘ê·¼í•  ë•Œ ì¸ì¦ í† í°ì´ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê²Œì´í‹°ë“œ ëª¨ë¸ì˜ ê²½ìš° ì ‘ê·¼ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # ëª¨ë¸ êµ¬ì„± íŒŒì¼ ê°€ì ¸ì˜¤ê¸°
    hf_config = fetch_model_config_from_hf(request.model_name, token=token)
    if hf_config is None:
        # ê²Œì´í‹°ë“œ ëª¨ë¸ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
        if "llama" in request.model_name.lower() or "mistral" in request.model_name.lower():
            if not token:
                warning_message = (
                    f"{request.model_name}ì€(ëŠ”) ê²Œì´í‹°ë“œ ëª¨ë¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                    "Hugging Face í† í°ì„ ì œê³µí•˜ì—¬ ì ‘ê·¼ ê¶Œí•œì„ ì–»ìœ¼ì„¸ìš”. "
                    "ëª¨ë¸ êµ¬ì„±ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ì¶”ì •ì¹˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
                )
                print(f"âš ï¸ {warning_message}")
        
        estimated_values.append("model_configuration")
    config_context_length = None
    if hf_config:
        config_context_length = get_context_length_from_config(hf_config)
        if config_context_length is None:
            estimated_values.append("context_length_from_config")

    # ì‹œí€€ìŠ¤ ê¸¸ì´ ê²°ì •
    if request.max_seq_len is not None:
        effective_max_seq_len = request.max_seq_len
        if config_context_length and effective_max_seq_len > config_context_length:
            raise HTTPException(
                status_code=400, 
                detail=f"ì…ë ¥í•œ max_seq_len({request.max_seq_len})ì´ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´({config_context_length})ë³´ë‹¤ í½ë‹ˆë‹¤. max_seq_lenì„ ëª¨ë¸ì˜ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë³´ë‹¤ ì‘ê²Œ ì„¤ì •í•˜ì„¸ìš”."
            )
    elif config_context_length:
        effective_max_seq_len = config_context_length
        estimated_values.append("context_length_from_config")
    else:
        raise HTTPException(
                status_code=400, 
                detail="ì‚¬ìš© ê°€ëŠ¥í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ Config íŒŒì¼ì—ëŠ” ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆì§€ ì•Šìœ¼ë¯€ë¡œ max_seq_lenì„ ì œê³µí•˜ì„¸ìš”."
        )
    
    if effective_max_seq_len <= 0:
        raise HTTPException(status_code=400, detail="max_seq_lenì€ ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤")

    # max_num_seqs í™•ì¸
    if request.max_num_seqs <= 0:
        raise HTTPException(status_code=400, detail="max_num_seqsëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
    
    # ë§¤ê°œë³€ìˆ˜ ë‹¨ìœ„ ë³€í™˜
    if request.params_millions is not None and request.params_billions is None:
        request.params_billions = request.params_millions / 1000.0
    
    # KV ìºì‹œ dtype ì„¤ì •
    if request.kv_cache_dtype is None:
        request.kv_cache_dtype = request.dtype
    
    # dtype ìœ íš¨ì„± í™•ì¸
    if request.dtype not in DTYPE_SIZES:
        raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” dtype: {request.dtype}. ì§€ì›ë˜ëŠ” ìœ í˜•: {list(DTYPE_SIZES.keys())}")
    
    if request.kv_cache_dtype not in DTYPE_SIZES:
        raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” KV ìºì‹œ dtype: {request.kv_cache_dtype}. ì§€ì›ë˜ëŠ” ìœ í˜•: {list(DTYPE_SIZES.keys())}")
    
    # ì–‘ìí™” ìœ íš¨ì„± í™•ì¸
    if request.quantization is not None and request.quantization not in QUANTIZATION_FACTORS:
        raise HTTPException(status_code=400, detail=f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì–‘ìí™”: {request.quantization}. ì§€ì›ë˜ëŠ” ìœ í˜•: {list(QUANTIZATION_FACTORS.keys())}")
    
    # ëª¨ë¸ ë©”ëª¨ë¦¬ ê³„ì‚°
    model_info = calculate_model_memory(
        request.model_name, 
        request.dtype, 
        hf_config,
        request.quantization,
        request.params_billions,
        request.params_millions
    )
    
    model_params_memory_gb = model_info["model_size_gb"]
    params_billions = model_info["params_billions"]
    
    # ë§¤ê°œë³€ìˆ˜ ìˆ˜ ì¶”ì • ì—¬ë¶€ í™•ì¸
    if request.params_billions is None and request.params_millions is None:
        if hf_config:
            params_from_config = get_params_from_hf_config(hf_config)
            if params_from_config is None:
                estimated_values.append("parameter_count")
        else:
            estimated_values.append("parameter_count")
    
    # ëª¨ë¸ ì•„í‚¤í…ì²˜ ê°€ì ¸ì˜¤ê¸°, ìœ ì € ì…ë ¥ ì•„í‚¤í…ì²˜ ìš°ì„  ì ìš©
    architecture = get_model_architecture(
        request.model_name,
        params_billions,
        hf_config,
        request.architecture
    )
    # ê° ë©”ëª¨ë¦¬ êµ¬ì„± ìš”ì†Œ ê³„ì‚°
    activation_memory_gb = calculate_activation_memory_inference(
        architecture,
        convert_max_num_seqs_to_base(request.max_num_seqs),
        effective_max_seq_len,
        request.dtype,
        request.model_name, # ëª¨ë¸ ë³„ í™œì„±í™” ë©”ëª¨ë¦¬ ìµœì í™” ê°œìˆ˜ í•˜ë“œ ì½”ë”©ìš©
        request.quantization # ì–‘ìí™” ì ìš©
    )
    
    kv_cache_memory = calculate_kv_cache_memory(
        architecture,
        effective_max_seq_len,
        convert_max_num_seqs_to_base(request.max_num_seqs),
        request.kv_cache_dtype,
        effective_max_seq_len
    )
    
    min_kv_cache_memory_gb = kv_cache_memory["min_kv_cache_memory_gb"]
    max_kv_cache_memory_gb = kv_cache_memory["max_kv_cache_memory_gb"]
    
    # CUDA ì˜¤ë²„í—¤ë“œ ì¶”ì •
    cuda_overhead_gb = 0.25
    # ì–‘ìí™” ì˜¤ë²„í—¤ë“œ ì¶”ì •
    if request.quantization:
        cuda_overhead_gb += 0.25
    
    # ì´ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ ê³„ì‚°
    total_min_memory_gb = (
        model_params_memory_gb + 
        activation_memory_gb + 
        min_kv_cache_memory_gb + 
        cuda_overhead_gb
    )
    
    total_max_memory_gb = (
        model_params_memory_gb + 
        activation_memory_gb + 
        max_kv_cache_memory_gb + 
        cuda_overhead_gb
    )
    
    recommended_memory_gb = total_min_memory_gb

    # GPUë‹¹ ë©”ëª¨ë¦¬ ê³„ì‚° (í…ì„œ ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
    per_gpu_memory = calculate_per_gpu_memory(
        request.tensor_parallel_size,
        model_params_memory_gb,
        activation_memory_gb,
        min_kv_cache_memory_gb,
        max_kv_cache_memory_gb,
        cuda_overhead_gb
    )
    
    per_gpu_total_min_memory_gb = round(
        per_gpu_memory["per_gpu_model_params"] +
        per_gpu_memory["per_gpu_activation"] +
        per_gpu_memory["per_gpu_min_kv_cache"] +
        per_gpu_memory["per_gpu_cuda_overhead"],
        2
    )
    
    per_gpu_total_max_memory_gb = round(
        per_gpu_memory["per_gpu_model_params"] +
        per_gpu_memory["per_gpu_activation"] +
        per_gpu_memory["per_gpu_max_kv_cache"] +
        per_gpu_memory["per_gpu_cuda_overhead"],
        2
    )
    
    # ê°’ ë°˜ì˜¬ë¦¼
    model_params_memory_gb = round(model_params_memory_gb, 2)
    activation_memory_gb = round(activation_memory_gb, 2)
    min_kv_cache_memory_gb = round(min_kv_cache_memory_gb, 2)
    max_kv_cache_memory_gb = round(max_kv_cache_memory_gb, 2)
    total_min_memory_gb = round(total_min_memory_gb, 2)
    total_max_memory_gb = round(total_max_memory_gb, 2)
    recommended_memory_gb = round(recommended_memory_gb, 2)
    
    for key in per_gpu_memory:
        per_gpu_memory[key] = round(per_gpu_memory[key], 2)
    
    # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì†ŒìŠ¤ ì„¤ì •
    context_length_source = "config"
    if request.max_seq_len is not None:
        context_length_source = "ì‚¬ìš©ì"
    elif "context_length_estimated" in estimated_values:
        context_length_source = "ì¶”ì •"
    
    # ë©”ëª¨ë¦¬ êµ¬ì„± ìš”ì†Œ ìš”ì•½
    components_breakdown = {
        "model_params": model_params_memory_gb,
        "activation": activation_memory_gb,
        "min_kv_cache": min_kv_cache_memory_gb,
        "max_kv_cache": max_kv_cache_memory_gb,
        "cuda_overhead": cuda_overhead_gb
    }

    # ê²½ê³  ë©”ì‹œì§€ ìƒì„±
    warning = None
    if estimated_values:
        if "parameter_count" in estimated_values and "model_architecture" in estimated_values:
            warning = "ì¤‘ìš”í•œ ëª¨ë¸ ì„¸ë¶€ ì •ë³´ ë° ì•„í‚¤í…ì²˜ê°€ ì¶”ì •ë˜ì–´ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì •í™•í•œ ê³„ì‚°ì„ ìœ„í•´ ì´ëŸ¬í•œ ê°’ì„ ìˆ˜ë™ìœ¼ë¡œ ì œê³µí•˜ëŠ” ê²ƒì„ ê³ ë ¤í•˜ì„¸ìš”.\n"
        elif "parameter_count" in estimated_values:
            warning = "ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ê°€ ëª¨ë¸ ì´ë¦„ì—ì„œ ì¶”ì •ë˜ì–´ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì •í™•í•œ ê³„ì‚°ì„ ìœ„í•´ params_billions ë˜ëŠ” params_millions ì œê³µì„ ê³ ë ¤í•˜ì„¸ìš”.\n"
        elif "model_architecture" in estimated_values:
            warning = "ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¸ë¶€ ì •ë³´ê°€ ì¶”ì •ë˜ì–´ ë¶€ì •í™•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì •í™•í•œ ê³„ì‚°ì„ ìœ„í•´ ì•„í‚¤í…ì²˜ ì„¸ë¶€ ì •ë³´ ì œê³µì„ ê³ ë ¤í•˜ì„¸ìš”.\n"
        
        # ê²Œì´í‹°ë“œ ëª¨ë¸ì— ëŒ€í•œ íŠ¹ë³„ ê²½ê³ 
        if "model_configuration" in estimated_values and ("llama" in request.model_name.lower() or "mistral" in request.model_name.lower()):
            if not token:
                if warning:
                    warning += " ì´ ëª¨ë¸ì€ ê²Œì´í‹°ë“œ ëª¨ë¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•´ Hugging Face í† í°ì„ ì œê³µí•˜ì„¸ìš”.\n"
                else:
                    warning = "ì´ ëª¨ë¸ì€ ê²Œì´í‹°ë“œ ëª¨ë¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë” ì •í™•í•œ ê²°ê³¼ë¥¼ ìœ„í•´ Hugging Face í† í°ì„ ì œê³µí•˜ì„¸ìš”.\n"
    if config_context_length and request.max_seq_len and request.max_seq_len > config_context_length:
        if warning:
            warning += (f"â„¹ï¸ ì…ë ¥í•œ ì…ë ¥ í† í° ìˆ˜({request.max_seq_len})ê°€ ëª¨ë¸ ìµœëŒ€ ì‹œí€¸ìŠ¤ ê¸¸ì´ {config_context_length}ë³´ë‹¤ ê¹ë‹ˆë‹¤.\n")
        else:
            warning = (f"â„¹ï¸ ì…ë ¥í•œ ì…ë ¥ í† í° ìˆ˜({request.max_seq_len})ê°€ ëª¨ë¸ ìµœëŒ€ ì‹œí€¸ìŠ¤ ê¸¸ì´ {config_context_length}ë³´ë‹¤ ê¹ë‹ˆë‹¤.\n")
    if missing_values:
        if warning:
            warning += f" ê²°ì •í•  ìˆ˜ ì—†ëŠ” ëˆ„ë½ëœ ê°’: {', '.join(missing_values)}."
        else:
            warning = f"ê²°ì •í•  ìˆ˜ ì—†ëŠ” ëˆ„ë½ëœ ê°’: {', '.join(missing_values)}."
    
    if request.tensor_parallel_size > 1:
        multi_gpu_warning = f"í…ì„œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•´ {request.tensor_parallel_size}ê°œì˜ GPUì— ê±¸ì³ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì´ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        if warning:
            warning += " " + multi_gpu_warning
        else:
            warning = multi_gpu_warning

    # GPU ë©”ëª¨ë¦¬ í™œìš© ê¶Œì¥ ì‚¬í•­
    recommended_gpu_memory_utilization = None
    if request.current_gpu_memory_size_gb:
        if request.current_gpu_memory_size_gb <= 0:
            raise HTTPException(status_code=400, detail="current_gpu_memory_size_gbëŠ” ì–‘ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
        
        if request.tensor_parallel_size > 1:
            util_ratio = per_gpu_total_min_memory_gb / request.current_gpu_memory_size_gb
        else:
            util_ratio = total_min_memory_gb / request.current_gpu_memory_size_gb
        
        if util_ratio > 0.95:
            if warning:
                warning += "ëª¨ë¸ì´ ì•ˆì „í•œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨(0.95)ë³´ë‹¤ ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ë” í° GPU ì‚¬ìš©, ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸° ë˜ëŠ” ì–‘ìí™” ì ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.\n"
            else:
                warning = "ëª¨ë¸ì´ ì•ˆì „í•œ GPU ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìœ¨(0.95)ë³´ë‹¤ ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. ë” í° GPU ì‚¬ìš©, ë°°ì¹˜ í¬ê¸° ì¤„ì´ê¸° ë˜ëŠ” ì–‘ìí™” ì ìš©ì„ ê³ ë ¤í•˜ì„¸ìš”.\n"
            recommended_gpu_memory_utilization = None
        else:
            recommended_gpu_memory_utilization = round(util_ratio + 0.02, 2) # 2% ê³¼ëŒ€ ì˜ˆì¸¡ TODO: ì¶”í›„ 2% ì˜¤ì°¨ í™•ì¸ í•„ìš”
            recommended_gpu_memory_utilization = min(recommended_gpu_memory_utilization, 0.95) # TODO: ì¶”í›„ ìµœëŒ€ê°’ ì„¤ì • í™•ì¸ í•„ìš”

    # ìµœì¢… ë©”ëª¨ë¦¬ ì¶”ì • ë°˜í™˜
    print(f"âœ… {request.model_name}ì— ëŒ€í•œ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ ì¶”ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    current_gpu_memory_use_gb = round(current_gpu_memory_use_gb + per_gpu_total_min_memory_gb, 2)

    return MemoryEstimation(
        model_name=request.model_name,
        model_params_memory_gb=model_params_memory_gb,
        activation_memory_gb=activation_memory_gb,
        min_kv_cache_memory_gb=min_kv_cache_memory_gb,
        max_kv_cache_memory_gb=max_kv_cache_memory_gb,
        cuda_overhead_gb=cuda_overhead_gb,
        total_min_memory_gb=total_min_memory_gb,
        total_max_memory_gb=total_max_memory_gb,
        recommended_memory_gb=recommended_memory_gb,
        recommended_gpu_memory_utilization=recommended_gpu_memory_utilization,
        warning=warning,
        components_breakdown=components_breakdown,
        architecture_used=architecture,
        context_length_source=context_length_source,
        context_length=effective_max_seq_len,
        dtype=request.dtype,
        kv_cache_dtype=request.kv_cache_dtype,
        quantization=request.quantization,
        max_num_seqs=request.max_num_seqs,
        estimated_values=estimated_values,
        missing_values=missing_values,
        tensor_parallel_size=request.tensor_parallel_size,
        per_gpu_memory_breakdown=per_gpu_memory,
        per_gpu_total_min_memory_gb=per_gpu_total_min_memory_gb,
        per_gpu_total_max_memory_gb=per_gpu_total_max_memory_gb,
        current_gpu_memory_size_gb=current_gpu_memory_size_gb,
        current_gpu_memory_use_gb=current_gpu_memory_use_gb,
        model_params_count=request.params_billions if request.params_billions else estimate_params_from_name(request.model_name),
        model_memory_total_use_ratio=round(current_gpu_memory_use_gb / current_gpu_memory_size_gb + 0.02, 2) # 2% ê³¼ëŒ€ ì˜ˆì¸¡ ì¶”ê°€ TODO: ì¶”í›„ 2% ì˜¤ì°¨ í™•ì¸ í•„ìš”
    )

@app.get("/")
def read_root():
    return {
        "info": "vLLM GPU ë©”ëª¨ë¦¬ ê³„ì‚°ê¸° API",
        "usage": "ëª¨ë¸ ì„¸ë¶€ ì •ë³´ì™€ í•¨ê»˜ /estimate-gpu-memoryì— POST ìš”ì²­",
        "example_request": {
            "model_name": "meta-llama/Llama-3-8b",
            "dtype": "float16",
            "tensor_parallel_size": 2
        },
        "features": [
            "Hugging Face Hubì—ì„œ ëª¨ë¸ êµ¬ì„±ì„ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤",
            "ê°€ëŠ¥í•œ ê²½ìš° ëª¨ë¸ êµ¬ì„±ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ìë™ìœ¼ë¡œ ê°ì§€í•©ë‹ˆë‹¤",
            "ì‚¬ìš©ì ì œê³µ ì•„í‚¤í…ì²˜ ì„¸ë¶€ ì •ë³´ ë° ì¬ì •ì˜ë¥¼ ì§€ì›í•©ë‹ˆë‹¤",
            "ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° ëª¨ë¸ ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ëŠ¥ì ìœ¼ë¡œ ì¶”ì •í•©ë‹ˆë‹¤",
            "vLLM ë°°í¬ë¥¼ ìœ„í•œ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ ê³„ì‚°í•©ë‹ˆë‹¤",
            "ì¶”ì •ë˜ê±°ë‚˜ ëˆ„ë½ëœ ê°’ì— ëŒ€í•œ ìì„¸í•œ í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤",
            "ì—¬ëŸ¬ GPUì— ê±¸ì¹œ í…ì„œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤",
            "ì¶”ì²œ GPU ë©”ëª¨ë¦¬ì–‘ì€ ëª¨ë¸ì— ìµœëŒ€ ì‹œí€¸ìŠ¤ ê¸¸ì´ * 0.2ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ë³´ë‹¤ ê¸´ ê¸¸ì´ì˜ ì‹œí€¸ìŠ¤ëŠ” ë³´ë‹¤ ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ìš”êµ¬í•©ë‹ˆë‹¤."
        ],
        "endpoints": {
            "/estimate-gpu-memory": "POST - GPU ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ ê³„ì‚°"
        },
        "notes": [
            "ì‹œí€€ìŠ¤ ê¸¸ì´(max_seq_len) ë§¤ê°œë³€ìˆ˜ëŠ” ì„ íƒ ì‚¬í•­ì…ë‹ˆë‹¤ - ì œê³µë˜ì§€ ì•Šìœ¼ë©´ APIëŠ” ëª¨ë¸ì˜ êµ¬ì„±ì—ì„œ ê°€ì ¸ì˜¤ë ¤ê³  ì‹œë„í•©ë‹ˆë‹¤",
            "ëª¨ë¸ì˜ êµ¬ì„±ì— ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° max_seq_lenì„ ì œê³µí•´ì•¼ í•©ë‹ˆë‹¤",
            "ë‹¤ì¤‘ GPU ì„¤ì •ì˜ ê²½ìš° GPUë‹¹ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ í™•ì¸í•˜ê¸° ìœ„í•´ 'tensor_parallel_size' ë§¤ê°œë³€ìˆ˜ë¥¼ ì§€ì •í•˜ì„¸ìš”",
            "ìš”ì²­ì— ìœ ì €ì˜ ê°’ì´ ìˆìœ¼ë©´ í•´ë‹¹ ê°’ì„ ìµœìš°ì„ ì ìœ¼ë¡œ ì ìš©í•©ë‹ˆë‹¤"
        ]
    }
@app.get("/get_gpu_catalog")
def get_gpu_catalog():
    from csv_to_json import csv_to_json_file, json_file_to_string
    try:
        csv_to_json_file(GPU_CSV_FILE_PATH, GPU_JSON_FILE_PATH)
        gpu_catalog = json_file_to_string(GPU_JSON_FILE_PATH)
        return gpu_catalog
    except Exception as e:
        print(f"Error loading csv file: {e}")
        return {}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)