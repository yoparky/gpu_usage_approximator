from utils.GeneralUtil import DTYPE_SIZES
from typing import Dict, Optional

def calculate_kv_cache_memory(arch: Dict[str, int], 
                             max_seq_len: int, 
                             max_num_seqs_ratio: int, 
                             kv_cache_dtype: str,
                             max_model_len: Optional[int] = None) -> Dict[str, float]:
    """
    PagedAttentionì„ ì‚¬ìš©í•œ KV ìºì‹œ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    í•´ë‹¹ ê³„ì‚°ì€ ë¡œê·¸ ìƒ vLLMì´ ì‚¬ìš©í•˜ëŠ” ë©”ëª¨ë¦¬ ì–‘ê³¼ ì •í™•íˆ ì¼ì¹˜í•©ë‹ˆë‹¤.
    
    Args:
        arch: ëª¨ë¸ ì•„í‚¤í…ì²˜ ë§¤ê°œë³€ìˆ˜
        max_seq_len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´
        max_num_seqs_ratio: ë°°ì¹˜ ë‚´ ë””í´íŠ¸ ì‹œí€¸ìŠ¤ ìˆ˜ ê¸°ì¤€ ë©”ëª¨ë¦¬ ì¦ê° ë¹„ìœ¨
        kv_cache_dtype: KV ìºì‹œì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„° íƒ€ì…
        max_model_len: ëª¨ë¸ì˜ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ (ë˜ëŠ” ì„œë¹™ì‹œ ì„¤ì •í•œ max_seq_len ì‚¬ìš©)
        
    Returns:
        KV ìºì‹œ ë©”ëª¨ë¦¬ ìš”êµ¬ ì‚¬í•­ì´ í¬í•¨ëœ ë”•ì…”ë„ˆë¦¬
    """
    print("\nğŸ’¾ KV ìºì‹œ ë©”ëª¨ë¦¬ ê³„ì‚° ì‹œì‘...")
    
    if max_model_len is None:
        max_model_len = max_seq_len
        print(f"â„¹ï¸ max_model_lenì´ ì„¤ì •ë˜ì§€ ì•ŠìŒ, max_seq_lenìœ¼ë¡œ ëŒ€ì²´: {max_seq_len}")
    
    # í•„ìš”í•œ ì•„í‚¤í…ì²˜ ê°’ ì¶”ì¶œ ë° ê²€ì¦
    num_layers = arch["num_layers"]
    num_heads = arch["num_heads"]
    head_dim = arch["head_dim"]
    
    # num_key_value_headsê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ì—†ìœ¼ë©´ num_heads ì‚¬ìš©
    if "num_key_value_heads" not in arch:
        print("âš ï¸ ì•„í‚¤í…ì²˜ì— num_key_value_headsê°€ ì—†ìŠµë‹ˆë‹¤. num_headsë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        num_key_value_heads = num_heads
    else:
        num_key_value_heads = arch["num_key_value_heads"]
    
    print(f"ğŸ“Š KV ìºì‹œ ê³„ì‚° ë§¤ê°œë³€ìˆ˜:")
    print(f"  - num_layers: {num_layers}")
    print(f"  - num_heads: {num_heads}")
    print(f"  - num_key_value_heads: {num_key_value_heads}")
    print(f"  - head_dim: {head_dim}")
    print(f"  - max_seq_len/max_model_len: {max_seq_len}/{max_model_len}")
    print(f"  - max_num_seqs_ratio: {max_num_seqs_ratio}")
    print(f"  - kv_cache_dtype: {kv_cache_dtype}")
    
    bytes_per_token = DTYPE_SIZES[kv_cache_dtype]
    print(f"  - bytes_per_token: {bytes_per_token}")
    
    # vLLM ê¸°ë³¸ ë¸”ë¡ í¬ê¸°
    block_size = 16
    if max_model_len > 8192:
        block_size = 32
    print(f"  - block_size: {block_size}")
    
    # ê³µì‹: 2 (KV ë‹¹ í•˜ë‚˜ ì”©) * num_key_value_heads * head_dim * num_layers * dtype_size * seq_len * batch_size (vLLM ì—ì„œ ë°°ì¹˜ ë‹¹ 256 seq ê¸°ì¤€ 1)
    kv_cache_size_bytes = 2 * num_key_value_heads * head_dim * num_layers * bytes_per_token * max_model_len #* max_num_seqs_ratio
    
    # GB ë‹¨ìœ„ë¡œ ë³€í™˜
    kv_cache_size_gb = kv_cache_size_bytes / (1024**3)
    
    print(f"ğŸ§® KV ìºì‹œ ë©”ëª¨ë¦¬ ê³„ì‚°:")
    print(f"  - ê³„ì‚°ì‹: 2 (KV ë‹¹ í•˜ë‚˜ ì”©) * {num_key_value_heads} * {head_dim} * {num_layers} * {bytes_per_token} * {max_model_len} * {max_num_seqs_ratio}")
    print(f"  - ì´ ë°”ì´íŠ¸: {kv_cache_size_bytes:,}")
    print(f"  - GB ë‹¨ìœ„: {kv_cache_size_gb:.2f} GB")
    
    result = {
        "min_kv_cache_memory_gb": kv_cache_size_gb,
        "max_kv_cache_memory_gb": kv_cache_size_gb
    }
    
    return result