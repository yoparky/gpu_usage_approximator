import json
import requests
from functools import lru_cache
from typing import Optional, Dict
from huggingface_hub import hf_hub_download
from utils.GeneralUtil import PARAM_MAPPINGS
from classes.ModelArchitecture import ModelArchitecture
from utils.GeneralUtil import estimate_model_architecture

@lru_cache(maxsize=100)
def fetch_model_config_from_hf(model_name: str, revision: str = None, token: Optional[str] = None) -> Optional[Dict]:
    """
    Hugging Face Hubì—ì„œ ëª¨ë¸ êµ¬ì„±ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        model_name: Hugging Face Hubì˜ ëª¨ë¸ ì´ë¦„
        revision: ë‹¤ìš´ë¡œë“œí•  íŠ¹ì • ë²„ì „ (ë¸Œëžœì¹˜, íƒœê·¸, ì»¤ë°‹ í•´ì‹œ)
        token: Hugging Face Hub ì ‘ê·¼ì„ ìœ„í•œ ì¸ì¦ í† í° (ê²Œì´í‹°ë“œ ëª¨ë¸ì— í•„ìš”)
        
    Returns:
        ëª¨ë¸ êµ¬ì„±ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° None
    """
    # í† í° ì‚¬ìš© ì—¬ë¶€ ë¡œê¹…
    if token:
        print(f"ðŸ” ì¸ì¦ í† í°ì„ ì‚¬ìš©í•˜ì—¬ {model_name} ì ‘ê·¼ì„ ì‹œë„í•©ë‹ˆë‹¤.")
    else:
        print(f"â„¹ï¸ í† í° ì—†ì´ {model_name} ì ‘ê·¼ì„ ì‹œë„í•©ë‹ˆë‹¤. ê²Œì´í‹°ë“œ ëª¨ë¸ì˜ ê²½ìš° ì ‘ê·¼ì´ ì œí•œë  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.")
    
    # ë¨¼ì € hf_hub_downloadë¥¼ ì‚¬ìš©í•˜ì—¬ config.json íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì‹œë„
    try:
        config_file = hf_hub_download(
            repo_id=model_name,
            filename="config.json",
            repo_type="model",
            revision=revision,
            cache_dir=None,  # ê¸°ë³¸ ìºì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©
            local_files_only=False,  # ì˜¨ë¼ì¸ ë¦¬ì†ŒìŠ¤ì— ì ‘ê·¼
            token=token,  # ì¸ì¦ í† í° ì¶”ê°€, ì§ì ‘ ìž…ë ¥
            etag_timeout=10,  # ETag íƒ€ìž„ì•„ì›ƒ ì„¤ì •
        )
        
        print(f"ðŸ“¥ {model_name}ì˜ config.json íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
                
            # êµ¬ì„± íŒŒì¼ì´ ìœ íš¨í•œì§€ í™•ì¸
            if not config or not isinstance(config, dict):
                print(f"âš ï¸ {model_name}ì˜ config.json íŒŒì¼ì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤.")
                return None
                
            print(f"âœ… {model_name}ì˜ êµ¬ì„± íŒŒì¼ íŒŒì‹± ì„±ê³µ.")
            
            # num_key_value_heads í™•ì¸ ë¡œê·¸ ì¶”ê°€
            if "num_key_value_heads" in config:
                print(f"ðŸ”‘ num_key_value_heads ë°œê²¬: {config['num_key_value_heads']}, GQA ëª¨ë¸ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
                
            return config
            
        except json.JSONDecodeError as e:
            print(f"âŒ {model_name}ì˜ êµ¬ì„± íŒŒì¼ì„ JSONìœ¼ë¡œ íŒŒì‹±í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {str(e)}")
    
    except Exception as e:
        # hf_hub_download ì‹¤íŒ¨ ì‹œ API ìš”ì²­ìœ¼ë¡œ ëŒ€ì²´
        print(f"âŒ hf_hub_downloadë¥¼ í†µí•œ {model_name} êµ¬ì„± ë‹¤ìš´ë¡œë“œì— ì‹¤íŒ¨: {str(e)}")
        
        # ê²Œì´í‹°ë“œ ëª¨ë¸ ê´€ë ¨ ì—ëŸ¬ í™•ì¸
        if "401" in str(e) or "403" in str(e) or "unauthorized" in str(e).lower() or "gated" in str(e).lower():
            print("â›” ì ‘ê·¼ ê±°ë¶€ë¨: ëª¨ë¸ì´ ê²Œì´í‹°ë“œ ìƒíƒœì´ë©° ìœ íš¨í•œ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            if not token:
                print("ðŸ’¡ í† í°ì„ ì œê³µí•˜ê±°ë‚˜, Hugging Faceì— ë¡œê·¸ì¸ë˜ì–´ ìžˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            else:
                print("ðŸ’¡ ì œê³µëœ í† í°ì´ ì˜¬ë°”ë¥¸ì§€, ë§Œë£Œë˜ì§€ ì•Šì•˜ëŠ”ì§€, ê·¸ë¦¬ê³  í•„ìš”í•œ ê¶Œí•œì´ ìžˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
            return None
        
        print("ðŸ”„ API ìš”ì²­ì„ í†µí•´ êµ¬ì„± ë‹¤ìš´ë¡œë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
        
        branch = "main" if revision is None else revision
        api_url = f"https://huggingface.co/{model_name}/raw/{branch}/config.json"
        try:
            headers = {"Authorization": f"Bearer {token}"} if token else {}
            response = requests.get(api_url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                try:
                    config = response.json()
                    
                    # API ì‘ë‹µì´ ìœ íš¨í•œì§€ í™•ì¸
                    if not config or not isinstance(config, dict):
                        print(f"âš ï¸ {model_name}ì˜ API ì‘ë‹µì´ ìœ íš¨í•˜ì§€ ì•Šê±°ë‚˜ ë¹„ì–´ ìžˆìŠµë‹ˆë‹¤.")
                        return None
                    
                    print(f"âœ… {model_name}ì˜ êµ¬ì„± íŒŒì¼ì„ APIë¥¼ í†µí•´ ì„±ê³µì ìœ¼ë¡œ ë‹¤ìš´ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
                    
                    # num_key_value_heads í™•ì¸ ë¡œê·¸ ì¶”ê°€
                    if "num_key_value_heads" in config:
                        print(f"ðŸ”‘ num_key_value_heads ë°œê²¬: {config['num_key_value_heads']}")
                        
                    return config
                    
                except json.JSONDecodeError:
                    print(f"âŒ {model_name}ì˜ API ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            elif response.status_code in [401, 403]:
                print(f"â›” ì ‘ê·¼ ê±°ë¶€ë¨ (HTTP {response.status_code}): ëª¨ë¸ì´ ê²Œì´í‹°ë“œ ìƒíƒœì´ë©° ìœ íš¨í•œ í† í°ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            else:
                print(f"âŒ {model_name}ì˜ êµ¬ì„± íŒŒì¼ì„ APIë¥¼ í†µí•´ ê°€ì ¸ì˜¤ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: HTTP {response.status_code}")
                
        except requests.exceptions.RequestException as e:
            print(f"âŒ {model_name}ì˜ API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
    return None

def get_params_from_hf_config(config: Dict) -> Optional[float]:
    """
    ê°€ëŠ¥í•œ ê²½ìš° Hugging Face êµ¬ì„±ì—ì„œ ë§¤ê°œë³€ìˆ˜ ìˆ˜ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        config: Hugging Faceì—ì„œ ê°€ì ¸ì˜¨ ëª¨ë¸ êµ¬ì„±ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ë§¤ê°œë³€ìˆ˜ ìˆ˜ë¥¼ ì‹­ì–µ ë‹¨ìœ„ë¡œ ë‚˜íƒ€ë‚´ëŠ” ë¶€ë™ ì†Œìˆ˜ì  ë˜ëŠ” ì§ì ‘ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° None
    """
    param_fields = [
        "num_parameters", 
        "n_params", 
        "num_params", 
        "total_params", 
        "parameter_count", 
        "model_parameters", 
        "total_parameter_count", 
        "params", 
        "n_parameters", 
        "param_count", 
        "parameters"
    ]
    
    for field in param_fields:
        if field in config:
            print(f"í•„ë“œ '{field}'ì—ì„œ ë§¤ê°œë³€ìˆ˜ ìˆ˜ ë°œê²¬: {config[field]}")
            return config[field] / 1e9
    
    return None


def parse_hf_config_to_architecture(config: Dict) -> Dict[str, int]:
    """
    Hugging Face config.jsonì„ ì•„í‚¤í…ì²˜ í˜•ì‹ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.
    ë‹¤ì–‘í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ì™€ íŠ¹ì • ëª…ëª… ê·œì¹™ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        config: Hugging Faceì—ì„œ ê°€ì ¸ì˜¨ ëª¨ë¸ êµ¬ì„±ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        
    Returns:
        í‘œì¤€í™”ëœ ì•„í‚¤í…ì²˜ ë§¤ê°œë³€ìˆ˜ë¥¼ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    """
    arch = {}
    
    # ì–´ë–¤ ê°’ì„ ì¶”ì¶œí• ì§€ ë¡œê¹…
    print("ðŸ” ëª¨ë¸ êµ¬ì„±ì—ì„œ ì•„í‚¤í…ì²˜ íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì¤‘...")
    
    # êµ¬ì„±ì—ì„œ ì¤‘ìš” ë§¤ê°œë³€ìˆ˜ ì¶”ì¶œ, YAML íŒŒì¼ì—ì„œ Huggingface ëª¨ë¸ config í‚¤ ì¶”ê°€ / ì œê±°
    for our_key, possible_keys in PARAM_MAPPINGS.items():
        for key in possible_keys:
            if key in config:
                # TODO: ì•„í‚¤í…ì³ ì‚¬ìš© ë°©ì‹ ë‹¨ìˆœí™” ê³ ë ¤
                # ì‚¬ìš©í•œ í‚¤ ì•„í‚¤í…ì³ì— ìž„ì‹œ ì €ìž¥
                arch[our_key] = config[key]
                print(f"âœ… {our_key} íŒŒë¼ë¯¸í„° ë°œê²¬: {config[key]} (ì›ë³¸ í‚¤: {key})")
                break
    
    # ëª¨ë¸ ìœ í˜• í™•ì¸
    model_type = config.get("model_type", "").lower()
    print(f"ðŸ“Œ ëª¨ë¸ ìœ í˜•: {model_type}")
    
    # head_dimì´ ì—†ëŠ” ê²½ìš° ê³„ì‚°
    # TODO: HEAD_DIM ê³„ì‚° ë°©ì‹ ë”ë¸”ì²´í¬
    if "head_dim" not in arch and "hidden_size" in arch and "num_heads" in arch:
        head_dim = arch["hidden_size"] // arch["num_heads"]
        if head_dim > 96:
            head_dim = round(head_dim / 32) * 32
        else:
            head_dim = 2 ** round(math.log2(head_dim))
        arch["head_dim"] = head_dim
        print(f"ðŸ§® head_dim ê³„ì‚°: {head_dim}")
    
    # intermediate_sizeê°€ ì—†ëŠ” ê²½ìš° ê³„ì‚°
    # TODO: INTERMEDIATE_SIZE ê³„ì‚° ë°©ì‹ ë”ë¸”ì²´í¬
    if "intermediate_size" not in arch and "hidden_size" in arch:
        if "llama" in model_type:
            multiplier = 2.75
        elif "mistral" in model_type:
            multiplier = 3.5
        else:
            multiplier = 4.0
            
        intermediate_size = int(arch["hidden_size"] * multiplier)
        intermediate_size = round(intermediate_size / 256) * 256
        arch["intermediate_size"] = intermediate_size
        print(f"ðŸ§® intermediate_size ê³„ì‚°: {intermediate_size} (ìŠ¹ìˆ˜: {multiplier})")
    
    # num_key_value_headsê°€ ì—†ëŠ” ê²½ìš° ì„¤ì •, ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì¡´ìž¬. ì—†ìœ¼ë©´ (GQA ì•„ë‹ ê²½ìš°) num_headsì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
    if "num_key_value_heads" not in arch and "num_heads" in arch:
        # GQA ë˜ëŠ” MQA ëª¨ë¸ì¸ì§€ í™•ì¸
        is_gqa_model = False
        
        if "llama" in model_type and "3" in config.get("_name_or_path", ""):
            is_gqa_model = True
            print("ðŸ” Llama 3 ëª¨ë¸ ê°ì§€ë¨, GQA êµ¬ì¡°ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
        
        if "rope_scaling" in config and "llama3" in config.get("rope_scaling", {}).get("rope_type", "").lower():
            is_gqa_model = True
            print("ðŸ” Llama 3 ë¡œí”„ ìŠ¤ì¼€ì¼ë§ ê°ì§€ë¨, GQA êµ¬ì¡°ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.")
            
        if is_gqa_model:
            # Llama 3 ëª¨ë¸ì˜ ê²½ìš° num_key_value_heads ì¶”ì •
            # ì°¸ê³ : ì´ê²ƒì€ ì¶”ì •ì¼ ë¿ì´ë©°, ì‹¤ì œ ê°’ì´ ë‹¤ë¥¼ ìˆ˜ ìžˆìŒ
            arch["num_key_value_heads"] = arch["num_heads"] // 3
            print(f"âš ï¸ num_key_value_heads ì¶”ì •: {arch['num_key_value_heads']} (Llama 3 GQA ì¶”ì •)")
        else:
            # ê¸°ë³¸ì ìœ¼ë¡œ num_headsì™€ ë™ì¼í•˜ê²Œ ì„¤ì •
            arch["num_key_value_heads"] = arch["num_heads"]
            print(f"â„¹ï¸ num_key_value_headsì„ num_headsì™€ ë™ì¼í•˜ê²Œ ì„¤ì •: {arch['num_heads']}")
    else:
        print(f"âœ… num_key_value_heads íŒŒë¼ë¯¸í„° ì‚¬ìš©: {arch['num_key_value_heads']}")
    
    # ì•„í‚¤í…ì²˜ ìš”ì•½
    print("ðŸ“‹ ìƒì„±ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ ìš”ì•½:")
    for key, value in arch.items():
        print(f"  - {key}: {value}")
    
    return arch


def get_model_architecture(model_name: str, 
                           params_billions: float, 
                           hf_config: dict = None,
                           user_arch: Optional[ModelArchitecture] = None) -> Dict[str, int]:
    """
    ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¸ë¶€ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ë©°, ìš°ì„ ìˆœìœ„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    1. ì‚¬ìš©ìž ì œê³µ ì•„í‚¤í…ì²˜ ì„¸ë¶€ ì •ë³´(ìµœìš°ì„ )
    2. Hugging Face êµ¬ì„±(ëŒ€ì²´)
    3. ë§¤ê°œë³€ìˆ˜ ìˆ˜ì— ë”°ë¥¸ ì¶”ì •(ìµœí›„ ìˆ˜ë‹¨)
    
    ì°¸ê³ : ì‚¬ìš©ìž ê°’ì€ í•­ìƒ HuggingFace ë˜ëŠ” ì¶”ì •ì—ì„œ ê°€ì ¸ì˜¨ ê°’ì„ ìž¬ì •ì˜í•©ë‹ˆë‹¤
    """
    arch = estimate_model_architecture(params_billions)
    
    if hf_config:
        hf_arch = parse_hf_config_to_architecture(hf_config)
        for key, value in hf_arch.items():
            if value is not None:
                arch[key] = value
    
    if user_arch:
        user_arch_dict = user_arch.dict(exclude_unset=True, exclude_none=True)
        for key, value in user_arch_dict.items():
            if value is not None:
                arch[key] = value
                print(f"ðŸ”‘ ì‚¬ìš©ìž ì œê³µ ì•„í‚¤í…ì²˜ë¡œ ë³€ê²½: {key} = {value}")
    
    return arch


def get_context_length_from_config(config: Dict) -> Optional[int]:
    """
    ëª¨ë¸ êµ¬ì„±ì—ì„œ ì»¨í…ìŠ¤íŠ¸ ì°½ í¬ê¸°ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ìœ ì €ì˜ ì§ì ‘ ìž…ë ¥ì´ ê³„ì‚°ì—
    ì‚¬ìš©ë©ë‹ˆë‹¤. ì°¨í›„ ì‚¬ìš©í•˜ê¸° ìœ„í•œ ì½”ë“œë¡œ í˜„ìž¬ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ 4096ì˜ ë””í´íŠ¸ê°’ì´ ì„¤ì •
    ë˜ì–´ìžˆìŠµë‹ˆë‹¤.
    
    Args:
        config: Hugging Faceì—ì„œ ê°€ì ¸ì˜¨ ëª¨ë¸ êµ¬ì„±ì„ í¬í•¨í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ìˆ˜ ë˜ëŠ” ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš° None
    """
    context_length_fields = PARAM_MAPPINGS['context_length']

    for field in context_length_fields:
        if field in config:
            return config[field]
    
    if "rope_scaling" in config:
        base_length = config.get("max_position_embeddings", 0)
        scaling_factor = config["rope_scaling"].get("factor", 1)
        if base_length and scaling_factor > 1:
            return int(base_length * scaling_factor)
    
    return None